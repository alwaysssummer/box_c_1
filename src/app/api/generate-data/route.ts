import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { createClient } from '@/lib/supabase/server'
import { createDetailedError, getAlternativeModel } from '@/lib/ai-errors'

// AI ëª¨ë¸ ì •ì˜ (test-promptì™€ ë™ì¼)
export const AI_MODELS = {
  'gpt-4o': { provider: 'openai', name: 'GPT-4o', description: 'ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸' },
  'gpt-4o-mini': { provider: 'openai', name: 'GPT-4o Mini', description: 'ë¹ ë¥´ê³  ì €ë ´' },
  'gpt-3.5-turbo': { provider: 'openai', name: 'GPT-3.5 Turbo', description: 'ê°€ì¥ ì €ë ´' },
  'claude-3-5-sonnet-20241022': { provider: 'anthropic', name: 'Claude 3.5 Sonnet', description: 'ê³ ì„±ëŠ¥' },
  'claude-3-haiku-20240307': { provider: 'anthropic', name: 'Claude 3 Haiku', description: 'ë¹ ë¦„' },
  'gemini-1.5-pro': { provider: 'google', name: 'Gemini 1.5 Pro', description: 'ê³ ì„±ëŠ¥, ê¸´ ì»¨í…ìŠ¤íŠ¸' },
  'gemini-2.0-flash': { provider: 'google', name: 'âš¡ Gemini 2.0 Flash (ì¶”ì²œ)', description: 'ë¹ ë¥´ê³  ì €ë ´í•œ ì¶”ì²œ ëª¨ë¸' },
  'gemini-2.5-flash': { provider: 'google', name: 'ğŸš€ Gemini 2.5 Flash (ìµœì‹ )', description: 'ìµœì‹  ê³ ì† ëª¨ë¸' },
} as const

type ModelId = keyof typeof AI_MODELS

// ============================================
// ìš”ì²­/ì‘ë‹µ íƒ€ì…
// ============================================

interface GenerateDataRequest {
  passageId: string          // ì§€ë¬¸ ID
  sentenceId?: string | null // ë¬¸ì¥ ID (ë¬¸ì¥ ë‹¨ìœ„ ìƒì„± ì‹œ)
  dataTypeId: string         // ë°ì´í„° ìœ í˜• ID
  model?: ModelId            // AI ëª¨ë¸ (ë¯¸ì§€ì • ì‹œ ë°ì´í„° ìœ í˜•ì˜ ì¶”ì²œ ëª¨ë¸ ì‚¬ìš©)
}

interface GenerateDataResponse {
  success: boolean
  data?: {
    id: string
    passageId: string
    sentenceId: string | null
    dataTypeId: string
    result: unknown
    status: string
    modelUsed: string
    confidence: number | null
    responseTime: number
    inputTokens: number
    outputTokens: number
  }
  error?: string
  aiError?: {
    type: string
    message: string
    solution: string
    severity?: string
    canRetry: boolean
    alternativeModel?: string | null
  }
}

// ============================================
// AI í˜¸ì¶œ í•¨ìˆ˜ë“¤
// ============================================

async function callOpenAI(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  })

  const response = await openai.chat.completions.create({
    model,
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt },
    ],
    temperature: 0.3, // ë°ì´í„° ìƒì„±ì€ ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ìœ ì§€
    response_format: { type: 'json_object' },
  })

  return {
    result: response.choices[0]?.message?.content || '',
    usage: {
      inputTokens: response.usage?.prompt_tokens || 0,
      outputTokens: response.usage?.completion_tokens || 0,
    },
  }
}

async function callAnthropic(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })

  const response = await anthropic.messages.create({
    model,
    max_tokens: 4096,
    system: systemPrompt,
    messages: [{ role: 'user', content: userPrompt }],
  })

  const textContent = response.content.find((c) => c.type === 'text')
  
  return {
    result: textContent?.type === 'text' ? textContent.text : '',
    usage: {
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
    },
  }
}

async function callGemini(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || '')
  const geminiModel = genAI.getGenerativeModel({ 
    model,
    systemInstruction: systemPrompt,
    generationConfig: {
      responseMimeType: 'application/json',
      temperature: 0.3,
    },
  })

  const result = await geminiModel.generateContent(userPrompt)
  const response = result.response
  const usageMetadata = response.usageMetadata
  
  return {
    result: response.text(),
    usage: {
      inputTokens: usageMetadata?.promptTokenCount || 0,
      outputTokens: usageMetadata?.candidatesTokenCount || 0,
    },
  }
}

// ============================================
// ë³€ìˆ˜ ì¹˜í™˜
// ============================================

function replaceVariables(
  prompt: string, 
  passage: { content: string; korean_translation: string | null },
  sentence?: { content: string; korean_translation: string | null }
): string {
  let result = prompt
  
  // ì§€ë¬¸ ë³€ìˆ˜
  result = result.replace(/\[\[passage\]\]/g, passage.content || '')
  result = result.replace(/\[\[korean\]\]/g, passage.korean_translation || '')
  
  // ë¬¸ì¥ ë³€ìˆ˜ (ë¬¸ì¥ ë‹¨ìœ„ ìƒì„± ì‹œ)
  if (sentence) {
    result = result.replace(/\[\[sentence\]\]/g, sentence.content || '')
    result = result.replace(/\[\[sentence_korean\]\]/g, sentence.korean_translation || '')
  }
  
  return result
}

// ============================================
// POST /api/generate-data - ë°ì´í„° ìƒì„±
// ============================================

export async function POST(request: NextRequest) {
  const startTime = Date.now()
  const supabase = await createClient()
  
  try {
    const body: GenerateDataRequest = await request.json()
    const { passageId, sentenceId, dataTypeId, model: requestedModel } = body

    // í•„ìˆ˜ í•„ë“œ ê²€ì¦
    if (!passageId || !dataTypeId) {
      return NextResponse.json<GenerateDataResponse>(
        { success: false, error: 'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤ (passageId, dataTypeId)' },
        { status: 400 }
      )
    }

    // 1. ë°ì´í„° ìœ í˜• ì¡°íšŒ (í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨)
    const { data: dataType, error: dataTypeError } = await supabase
      .from('data_types')
      .select(`
        *,
        prompt:prompts!data_types_prompt_id_fkey (
          id,
          content,
          output_schema,
          variables
        )
      `)
      .eq('id', dataTypeId)
      .single()

    if (dataTypeError || !dataType) {
      return NextResponse.json<GenerateDataResponse>(
        { success: false, error: 'ë°ì´í„° ìœ í˜•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' },
        { status: 404 }
      )
    }

    // 2. í”„ë¡¬í”„íŠ¸ í™•ì¸
    const promptContent = dataType.prompt?.content || dataType.prompt
    if (!promptContent) {
      return NextResponse.json<GenerateDataResponse>(
        { success: false, error: 'ë°ì´í„° ìœ í˜•ì— ì—°ê²°ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤' },
        { status: 400 }
      )
    }

    // 3. ì§€ë¬¸ ì¡°íšŒ
    const { data: passage, error: passageError } = await supabase
      .from('passages')
      .select('id, content, korean_translation')
      .eq('id', passageId)
      .single()

    if (passageError || !passage) {
      return NextResponse.json<GenerateDataResponse>(
        { success: false, error: 'ì§€ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' },
        { status: 404 }
      )
    }

    // 4. ë¬¸ì¥ ì¡°íšŒ (ë¬¸ì¥ ë‹¨ìœ„ ìƒì„± ì‹œ)
    let sentence = null
    if (sentenceId) {
      const { data: sentenceData, error: sentenceError } = await supabase
        .from('sentences')
        .select('id, content, korean_translation')
        .eq('id', sentenceId)
        .single()

      if (sentenceError || !sentenceData) {
        return NextResponse.json<GenerateDataResponse>(
          { success: false, error: 'ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' },
          { status: 404 }
        )
      }
      sentence = sentenceData
    }

    // 5. ëª¨ë¸ ê²°ì •
    const model = requestedModel || (dataType.recommended_model as ModelId) || 'gpt-4o-mini'
    const modelInfo = AI_MODELS[model]
    
    if (!modelInfo) {
      return NextResponse.json<GenerateDataResponse>(
        { success: false, error: `ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: ${model}` },
        { status: 400 }
      )
    }

    // 6. í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
    const processedPrompt = replaceVariables(promptContent, passage, sentence || undefined)
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    let systemPrompt = 'ë‹¹ì‹ ì€ ì˜ì–´ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'
    const outputSchema = dataType.prompt?.output_schema || dataType.output_schema
    if (outputSchema) {
      systemPrompt += `\n\në‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì—„ê²©íˆ ë”°ë¥´ì„¸ìš”:\n${typeof outputSchema === 'string' ? outputSchema : JSON.stringify(outputSchema, null, 2)}`
    }

    // 7. AI í˜¸ì¶œ
    let aiResult: { result: string; usage: { inputTokens: number; outputTokens: number } }

    switch (modelInfo.provider) {
      case 'openai':
        if (!process.env.OPENAI_API_KEY) {
          throw new Error('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        aiResult = await callOpenAI(model, systemPrompt, processedPrompt)
        break
        
      case 'anthropic':
        if (!process.env.ANTHROPIC_API_KEY) {
          throw new Error('ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        aiResult = await callAnthropic(model, systemPrompt, processedPrompt)
        break
        
      case 'google':
        if (!process.env.GOOGLE_GEMINI_API_KEY) {
          throw new Error('GOOGLE_GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        aiResult = await callGemini(model, systemPrompt, processedPrompt)
        break
        
      default:
        throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” ì œê³µì—…ì²´: ${modelInfo.provider}`)
    }

    const responseTime = Date.now() - startTime

    // 8. JSON íŒŒì‹±
    let parsedResult: unknown
    try {
      parsedResult = JSON.parse(aiResult.result)
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥
      parsedResult = { raw_text: aiResult.result }
    }

    // 9. DBì— ì €ì¥ (UPSERT)
    const { data: savedData, error: saveError } = await supabase
      .from('generated_data')
      .upsert({
        passage_id: passageId,
        sentence_id: sentenceId || null,
        data_type_id: dataTypeId,
        result: parsedResult,
        status: 'completed',
        model_used: model,
        confidence: 0.95, // TODO: AI ì‘ë‹µì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ
        response_time: responseTime,
        input_tokens: aiResult.usage.inputTokens,
        output_tokens: aiResult.usage.outputTokens,
        error_message: null,
      }, {
        onConflict: 'passage_id,data_type_id,sentence_id',
        ignoreDuplicates: false,
      })
      .select()
      .single()

    if (saveError) {
      console.error('Error saving generated data:', saveError)
      // ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜
    }

    return NextResponse.json<GenerateDataResponse>({
      success: true,
      data: {
        id: savedData?.id || '',
        passageId,
        sentenceId: sentenceId || null,
        dataTypeId,
        result: parsedResult,
        status: 'completed',
        modelUsed: model,
        confidence: 0.95,
        responseTime,
        inputTokens: aiResult.usage.inputTokens,
        outputTokens: aiResult.usage.outputTokens,
      },
    })

  } catch (error) {
    const responseTime = Date.now() - startTime
    
    // ì—ëŸ¬ ì •ë³´ ì¶”ì¶œ
    const body = await request.clone().json().catch(() => ({}))
    const model = body.model || 'gpt-4o-mini'
    
    // ìƒì„¸ ì—ëŸ¬ ë¶„ë¥˜
    const detailedError = createDetailedError(error, {
      model,
      provider: AI_MODELS[model as ModelId]?.provider,
      action: 'data-generation'
    })
    
    const alternativeModel = getAlternativeModel(model, detailedError.errorInfo.type)
    
    console.error('Data generation error:', {
      errorType: detailedError.errorInfo.type,
      message: detailedError.errorInfo.message,
      originalError: detailedError.originalError,
      model,
    })

    // ì‹¤íŒ¨ ìƒíƒœë¡œ DB ì €ì¥ ì‹œë„
    if (body.passageId && body.dataTypeId) {
      const supabase = await createClient()
      await supabase
        .from('generated_data')
        .upsert({
          passage_id: body.passageId,
          sentence_id: body.sentenceId || null,
          data_type_id: body.dataTypeId,
          status: 'failed',
          error_message: detailedError.errorInfo.message,
          model_used: model,
          response_time: responseTime,
        }, {
          onConflict: 'passage_id,data_type_id,sentence_id',
          ignoreDuplicates: false,
        })
    }
    
    return NextResponse.json<GenerateDataResponse>({
      success: false,
      error: `${detailedError.errorInfo.icon} ${detailedError.errorInfo.message}`,
      aiError: {
        type: detailedError.errorInfo.type,
        message: detailedError.errorInfo.message,
        solution: detailedError.errorInfo.solution,
        severity: detailedError.errorInfo.severity,
        canRetry: detailedError.errorInfo.canRetry,
        alternativeModel,
      },
    })
  }
}

// ============================================
// GET /api/generate-data - ìƒì„±ëœ ë°ì´í„° ì¡°íšŒ
// ============================================

export async function GET(request: NextRequest) {
  try {
    const supabase = await createClient()
    const { searchParams } = new URL(request.url)
    
    const passageId = searchParams.get('passageId')
    const dataTypeId = searchParams.get('dataTypeId')
    const status = searchParams.get('status')
    
    let query = supabase
      .from('generated_data')
      .select(`
        *,
        passage:passages!generated_data_passage_id_fkey (
          id, name, content
        ),
        data_type:data_types!generated_data_data_type_id_fkey (
          id, name, target
        ),
        sentence:sentences!generated_data_sentence_id_fkey (
          id, sentence_no, content
        )
      `)
      .order('created_at', { ascending: false })
    
    if (passageId) {
      query = query.eq('passage_id', passageId)
    }
    
    if (dataTypeId) {
      query = query.eq('data_type_id', dataTypeId)
    }
    
    if (status) {
      query = query.eq('status', status)
    }
    
    const { data, error } = await query
    
    if (error) {
      throw error
    }
    
    return NextResponse.json({ success: true, data })
    
  } catch (error) {
    console.error('Error fetching generated data:', error)
    return NextResponse.json(
      { success: false, error: 'Failed to fetch generated data' },
      { status: 500 }
    )
  }
}

