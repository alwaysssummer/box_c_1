import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { createDetailedError, getAlternativeModel } from '@/lib/ai-errors'

// AI ëª¨ë¸ ì •ì˜
export const AI_MODELS = {
  // OpenAI
  'gpt-4o': { provider: 'openai', name: 'GPT-4o', description: 'ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸' },
  'gpt-4o-mini': { provider: 'openai', name: 'GPT-4o Mini', description: 'ë¹ ë¥´ê³  ì €ë ´' },
  'gpt-3.5-turbo': { provider: 'openai', name: 'GPT-3.5 Turbo', description: 'ê°€ì¥ ì €ë ´' },
  // Anthropic
  'claude-3-5-sonnet-20241022': { provider: 'anthropic', name: 'Claude 3.5 Sonnet', description: 'ê³ ì„±ëŠ¥' },
  'claude-3-haiku-20240307': { provider: 'anthropic', name: 'Claude 3 Haiku', description: 'ë¹ ë¦„' },
  // Google
  'gemini-1.5-pro': { provider: 'google', name: 'Gemini 1.5 Pro', description: 'ê³ ì„±ëŠ¥, ê¸´ ì»¨í…ìŠ¤íŠ¸' },
  'gemini-2.0-flash': { provider: 'google', name: 'âš¡ Gemini 2.0 Flash (ì¶”ì²œ)', description: 'ë¹ ë¥´ê³  ì €ë ´í•œ ì¶”ì²œ ëª¨ë¸' },
  'gemini-2.5-flash': { provider: 'google', name: 'ğŸš€ Gemini 2.5 Flash (ìµœì‹ )', description: 'ìµœì‹  ê³ ì† ëª¨ë¸' },
} as const

type ModelId = keyof typeof AI_MODELS

interface TestPromptRequest {
  model: ModelId
  systemPrompt?: string
  userPrompt: string
  sampleInput: string
  outputSchema?: string
}

interface AIErrorDetail {
  type: string
  message: string
  solution: string
  severity?: string
  canRetry: boolean
  alternativeModel?: string | null
}

interface TestPromptResponse {
  success: boolean
  result?: string
  error?: string
  aiError?: AIErrorDetail  // ìƒì„¸ ì—ëŸ¬ ì •ë³´
  usage?: {
    inputTokens: number
    outputTokens: number
    totalTokens: number
  }
  responseTime: number
  model: string
}

// í”„ë¡¬í”„íŠ¸ì—ì„œ ë³€ìˆ˜ ì¹˜í™˜
function replaceVariables(prompt: string, sampleInput: string): string {
  return prompt
    .replace(/\[\[passage\]\]/g, sampleInput)
    .replace(/\[\[sentence\]\]/g, sampleInput)
    .replace(/\[\[korean\]\]/g, sampleInput)
    .replace(/\[\[input\]\]/g, sampleInput)
}

// OpenAI í˜¸ì¶œ
async function callOpenAI(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  })

  const response = await openai.chat.completions.create({
    model,
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt },
    ],
    temperature: 0.7,
  })

  return {
    result: response.choices[0]?.message?.content || '',
    usage: {
      inputTokens: response.usage?.prompt_tokens || 0,
      outputTokens: response.usage?.completion_tokens || 0,
    },
  }
}

// Anthropic í˜¸ì¶œ
async function callAnthropic(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })

  const response = await anthropic.messages.create({
    model,
    max_tokens: 4096,
    system: systemPrompt,
    messages: [{ role: 'user', content: userPrompt }],
  })

  const textContent = response.content.find((c) => c.type === 'text')
  
  return {
    result: textContent?.type === 'text' ? textContent.text : '',
    usage: {
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
    },
  }
}

// Google Gemini í˜¸ì¶œ
async function callGemini(
  model: string,
  systemPrompt: string,
  userPrompt: string
): Promise<{ result: string; usage: { inputTokens: number; outputTokens: number } }> {
  const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || '')
  const geminiModel = genAI.getGenerativeModel({ 
    model,
    systemInstruction: systemPrompt,
  })

  const result = await geminiModel.generateContent(userPrompt)
  const response = result.response
  
  // GeminiëŠ” í† í° ì‚¬ìš©ëŸ‰ì„ ë‹¤ë¥´ê²Œ ì œê³µ
  const usageMetadata = response.usageMetadata
  
  return {
    result: response.text(),
    usage: {
      inputTokens: usageMetadata?.promptTokenCount || 0,
      outputTokens: usageMetadata?.candidatesTokenCount || 0,
    },
  }
}

// POST /api/test-prompt - í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
export async function POST(request: NextRequest) {
  const startTime = Date.now()
  let model: string = ''
  
  try {
    const body: TestPromptRequest = await request.json()
    const { systemPrompt, userPrompt, sampleInput, outputSchema } = body
    model = body.model

    if (!model || !userPrompt || !sampleInput) {
      return NextResponse.json(
        { success: false, error: 'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤ (model, userPrompt, sampleInput)' },
        { status: 400 }
      )
    }

    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const modelInfo = (AI_MODELS as any)[model] as { provider: string; name: string; description: string } | undefined
    if (!modelInfo) {
      return NextResponse.json(
        { success: false, error: `ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: ${model}` },
        { status: 400 }
      )
    }

    // ë³€ìˆ˜ ì¹˜í™˜
    const processedPrompt = replaceVariables(userPrompt, sampleInput)
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    let finalSystemPrompt = systemPrompt || 'ë‹¹ì‹ ì€ ì˜ì–´ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.'
    if (outputSchema) {
      finalSystemPrompt += `\n\në°˜ë“œì‹œ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:\n${outputSchema}`
    }

    let result: { result: string; usage: { inputTokens: number; outputTokens: number } }

    // ì œê³µì—…ì²´ë³„ í˜¸ì¶œ
    switch (modelInfo.provider) {
      case 'openai':
        if (!process.env.OPENAI_API_KEY) {
          throw new Error('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        result = await callOpenAI(model, finalSystemPrompt, processedPrompt)
        break
        
      case 'anthropic':
        if (!process.env.ANTHROPIC_API_KEY) {
          throw new Error('ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        result = await callAnthropic(model, finalSystemPrompt, processedPrompt)
        break
        
      case 'google':
        if (!process.env.GOOGLE_GEMINI_API_KEY) {
          throw new Error('GOOGLE_GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
        }
        result = await callGemini(model, finalSystemPrompt, processedPrompt)
        break
        
      default:
        throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” ì œê³µì—…ì²´: ${modelInfo.provider}`)
    }

    const responseTime = Date.now() - startTime

    const response: TestPromptResponse = {
      success: true,
      result: result.result,
      usage: {
        inputTokens: result.usage.inputTokens,
        outputTokens: result.usage.outputTokens,
        totalTokens: result.usage.inputTokens + result.usage.outputTokens,
      },
      responseTime,
      model: modelInfo.name,
    }

    return NextResponse.json(response)
  } catch (error) {
    const responseTime = Date.now() - startTime
    
    // ìƒì„¸ ì—ëŸ¬ ë¶„ë¥˜
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const detailedError = createDetailedError(error, {
      model,
      provider: (AI_MODELS as any)[model]?.provider,
      action: 'prompt-test'
    })
    
    // ëŒ€ì•ˆ ëª¨ë¸ ì¶”ì²œ
    const alternativeModel = model ? getAlternativeModel(model, detailedError.errorInfo.type) : null
    
    console.error('Prompt test error:', {
      errorType: detailedError.errorInfo.type,
      message: detailedError.errorInfo.message,
      originalError: detailedError.originalError,
      model,
    })
    
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const modelName = (AI_MODELS as any)[model]?.name || ''
    return NextResponse.json({
      success: false,
      error: `${detailedError.errorInfo.icon} ${detailedError.errorInfo.message}`,
      aiError: {
        type: detailedError.errorInfo.type,
        message: detailedError.errorInfo.message,
        solution: detailedError.errorInfo.solution,
        severity: detailedError.errorInfo.severity,
        canRetry: detailedError.errorInfo.canRetry,
        alternativeModel,
      },
      responseTime,
      model: modelName,
    })
  }
}

// GET /api/test-prompt - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
export async function GET() {
  const models = Object.entries(AI_MODELS).map(([id, info]) => ({
    id,
    ...info,
    available: (() => {
      switch (info.provider) {
        case 'openai': return !!process.env.OPENAI_API_KEY
        case 'anthropic': return !!process.env.ANTHROPIC_API_KEY
        case 'google': return !!process.env.GOOGLE_GEMINI_API_KEY
        default: return false
      }
    })(),
  }))

  return NextResponse.json({ models })
}



